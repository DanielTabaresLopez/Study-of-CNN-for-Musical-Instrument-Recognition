{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP7wtKf6Dh6K"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directory where your audio files are stored\n",
        "audio_dir = 'path_to_your_audio_files/'\n",
        "\n",
        "# List of instrument labels\n",
        "instrument_labels = ['Piano', 'Clarinet', 'AcGuitar', 'ElGuitar', 'Saxophone', 'Keyboard', 'Violin', 'Trumpet', 'Voice']\n",
        "\n",
        "# Function to convert audio to Mel spectrogram\n",
        "def audio_to_mel_spectrogram(audio_file, sr=22050, n_mels=128, duration=5, offset=0):\n",
        "    y, sr = librosa.load(audio_file, sr=sr, duration=duration, offset=offset)\n",
        "    mel_spec = librosa.feature.melspectrogram(y, sr=sr, n_mels=n_mels, fmax=8000)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "    return mel_spec_db\n",
        "\n",
        "# Prepare the dataset\n",
        "def prepare_dataset(audio_dir, labels, img_size=(128, 128)):\n",
        "    images = []\n",
        "    labels_list = []\n",
        "\n",
        "    for label in labels:\n",
        "        label_dir = os.path.join(audio_dir, label)\n",
        "        for audio_file in os.listdir(label_dir):\n",
        "            audio_path = os.path.join(label_dir, audio_file)\n",
        "            mel_spec_db = audio_to_mel_spectrogram(audio_path)\n",
        "\n",
        "            # Resize spectrogram to match CNN input size (img_size)\n",
        "            mel_spec_resized = cv2.resize(mel_spec_db, img_size)  # cv2 used to resize the image\n",
        "\n",
        "            images.append(mel_spec_resized)\n",
        "            labels_list.append(labels.index(label))\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    images = np.array(images)\n",
        "    labels_list = np.array(labels_list)\n",
        "\n",
        "    # Normalize image values\n",
        "    images = images / np.max(images)\n",
        "\n",
        "    return images, labels_list\n",
        "\n",
        "# Prepare dataset\n",
        "X, y = prepare_dataset(audio_dir, instrument_labels)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.InputLayer(input_shape=(128, 128, 1)),  # Image shape (128x128, 1 channel)\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(len(instrument_labels), activation='softmax')  # Output layer for classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Save the model\n",
        "model.save('instrument_classification_model.h5')"
      ]
    }
  ]
}